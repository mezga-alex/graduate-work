% В этом файле следует писать текст работы, разбивая его на
% разделы (section), подразделы (subsection) и, если нужно,
% главы (chapter).

% Предварительно следует указать необходимую информацию
% в файле SETUP.tex

\input{preamble.tex}

\begin{document}

\Intro

Принято считать, что история развития обработки естественного языка берёт своё начало в 1950-х годах, когда Алан Тьюринг опубликовал свою работу <<Вычислительные машины и разум>> (англ. Computing Machinery and Intelligence), где был представлен тест Тьюринга[].

В настоящий момент в рамках обработки естественного языка (англ. Natural Language Processing), далее NLP, стоящей на пересечении таких компьютерных наук, как машинное обучение и компьютерная лингвистика, решаются проблемы анализа, понимания и извлечения смысла из естественной человеческой речи.

К наиболее актуальным задачам данной области можно отнести автоматическую суммаризация текстов, машинный перевод, распознавание именованных объектов, извлечение отношений, анализ настроений, распознавание речи и т.д.

Анализируя рынок онлайн-образования, можно отметить, что изучение иностранных языков является приоритетным направлением для потребителей. Создатели онлайн-платформ часто называют персонализацию обучения как отличительную особенность их обучения(?).

Рассмотрим основные этапы изучения грамматических явлений, встречающихся при знакомстве с новым языком:

\begin{itemize}
  \item Presentation \\ Демонстрация примеров и структуры грамматической конструкции.
  \item Pattern Recognition\\Самостоятельный поиск грамматических конструкций в исходном тексте.
 \item Controlled Practice\\Построение изучаемых грамматических конструкций в ограниченной форме.
 \item Semi-Controlled Practice\\Персонализация языка с упором на изучаемые грамматические явления.
 \item Free Practice\\Свободное использование языка.
\end{itemize}

Однако несмотря на большую гибкость по сравнению с классическим методом изучения иностранного языка в классах, некоторые материалы могут также быть устаревшими и неактуальными. Это связано с тем, что процесс формирования упражнений является времязатратным ввиду того, что задействует человеческий труд.

В рамках данной работы будет рассмотрена задача проектирования и реализации системы, позволяющей автоматизировать процесс создания заданий видов Controlled Practice и Semi-Controlled Practice, что позволит сократить время подготовки методических материалов и даст возможность большей персонализации траектории обучения.
\newpage
% Если typeOfWork в SETUP.tex задан как 2 или 3, то начинать
% надо не с section (раздел), а с главы (chapter)
\section{Теоретические основы задачи обработки естественного языка}
\label{sec:examples}

Задача обработки естественного языка это целый ряд теоретико-мотивированных вычислительных методов, которые позволяют производить анализ человеческого языка. В рамках NLP решается большое количество задач, затрагивающих различные уровни- начиная от определения частей речи (англ. Parts Of Speech), далее POS, заканчивая созданием диалоговых систем.

Долгое время в задачах NLP применялись модели поверхностного обучения, такие как SVM (англ. support vector machine), обученные на разреженных данных, представленных в пространстве высокой размерности, что не позволяло достичь достаточной точности и требовало значительных вычислительных ресурсов. Однако в последние годы были разработаны новые методы векторного представления слов, что позволило избежать экспоненциального роста размерности. Это помогло сократить время обучения сетей и перейти к методам, основанным на глубоком обучении, которые обеспечивают автоматическое обучение признакам. Это существенно отличает новые архитектуры от тех, что применялись раньше, так как для них больше не требуется ручное конструирование признаков.

Одна из первых архитектур глубокого обучения в области NLP была продемонстрирована в статье “Natural Language Processing (Almost) from Scratch” [] Ронаном Коллобертом и Джейсоном Уэстоном. На тот момент такая архитектура превосходила большую часть уже существующих моделей в поиска именованных сущностей (англ. named-entity recognition, NER), семантической маркировке ролей (англ. semantic role labeling, SRL), и определении частей речи (POS). С того времени появилось большое количество алгоритмов и моделей, решающих сложные задачи обработки естественного языка.

\subsection{Word Embeddings}
Статистические модели стали основным инструментом в задачах NLP, однако в начале они страдали от т.н. проклятия размерности[]. Это повлекло развитие алгоритмов, которые позволили бы представлять слова в низкоразмерном пространстве[].

Стоит сказать, что на данный момент нет общепризнанного перевода термина embedding (от англ. ‘вложение’), поэтому будет использоваться англицизм. 

Embedding представляет собой преобразование некой сущности в числовой вектор. Далее будут рассмотрены основные подходы, применяющиеся для решения данной задачи.

\subsubsection{One-hot encoding}

Одним из первых решений задачи векторного представления слов был так называемый унитарный код. Он представлял собой вектор длины n, которая определяется количеством слов некоторого словаря, содержащий (n-1) нулей и 1 единицу. Индекс значащей единицы соответствовал расположению слова в данном словаре ---см. таблицу~\ref{tab:bow-ohe}.

\begin{table}
\centering
\caption{\label{tab:bow-ohe}One-Hot Encoded векторы}
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
Vocabulary   & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \midrule
bag          & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
words        & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
model        & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
way          & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0  \\
representing & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0  \\
text         & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0  \\
data         & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0  \\
simple       & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0  \\
understand   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0  \\
implement    & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1  \\ \bottomrule
\end{tabular}
\end{table}

Несмотря на то, что такая архитектура позволяет решить проблему кодирования слов, она обладает рядом существенных недостатков:
\begin{itemize}
  \item При добавлении нового слова в середину существующего словаря есть необходимость заново проводить нумерацию его элементов.
  \item Происходит быстрый рост размерности представления текстов. \\ Например для текста из 9 уникальных слов требуется матрица 9х9
 \item Данный метод не предоставляет информации о семантической близости слов. Данный аспект связан с тем, что написание слов не имеет непосредственной связи с объектами, которые они описывают [2.12. Фердинанд де Соссюр (1827–1913). Лингвистический структурализм].
\end{itemize}


\subsubsection{Bag of Words}
На основе кодирования слов унитарным кодом, рассмотренным в пункте 1.1.1, был предложен более экономичный вариант представления текстов, называемый “мешком слов” (от англ. Bag of words).

Алгоритм построения такого представления содержит следующие основные этапы:
\begin{itemize}
  \item Предварительное создание словаря методом OHE.
  \item Кодирование слов, содержащихся в тексте.
 \item Сложение всех полученных one-hot векторов.
\end{itemize}

На выходе получим числовой вектор, который описывает информацию о количестве различных слов в исходном тексте.
Такая модель не сохраняет структуру входных данных, в связи с чем теряется информация о взаимном расположении слов.
Тем не менее, применяя данный подход можно сравнивать тексты путем сравнения выходных векторов. Примером такой метрики является косинусная мера:

\begin{equation}\label{eq:cos}
{\displaystyle {\text{similarity}}=\cos(\theta )={\mathbf {A} \cdot \mathbf {B}  \over \|\mathbf {A} \|\|\mathbf {B} \|}={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}{\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}},}
\end{equation}

$A_{i}$, $B_{i}$ -- компоненты векторов $\mathbf {A} $ и $ \mathbf {B}$ соответственно.\\

\begin{table}
\centering
\caption{\label{tab:bow-bow}Терм-документная таблица}
\begin{tabular}{@{}lcc@{}}
\toprule
Vocabulary   & \multicolumn{2}{l}{Documents} \\ \midrule
             	    & C1            & C2            \\ \midrule
bag          	    & 1               & 1             \\
words           & 1               & 1             \\
model           & 1               & 1             \\
way              & 1               & 0             \\
representing& 1               & 0             \\
text              & 1               & 0             \\
data             & 1               & 0             \\
simple           & 0               & 1             \\
understand   & 0               & 1             \\
implement    & 0               & 1             \\ \bottomrule
\end{tabular}
\end{table}

Рассмотрим этот процесс имплементации алгоритма.
\begin{enumerate}

  \item Зададим экземпляры текста.\\C1 = \textit{“The Bag of Words model is a way of representing text data.”}\\
	C2 = \textit{“The Bag of Words model is simple to understand and implement.”}
  \item Очистим тексты от пунктуационных символов и ‘стоп-слов’\footnote{the, of, is, a, to, and}, которые не несут смысловой нагрузки. 
   \item Сформируем словарь и закодируем его методом OHE (см. таблицу~\ref{tab:bow-ohe}).

  \item Для каждого предложения сложим One-Hot векторы слов, которые входят в их составы (см. таблицу~\ref{tab:bow-bow}). 
  
  \item Применим косинусную меру \eqref{eq:cos} к полученным векторам, которые количественно описывают исходные тексты.
\begin{equation*}
{\displaystyle {\text{similarity}}=\cos(\theta )= \frac{3} { \sqrt{7} \cdot  \sqrt{6}}= {0.46291}}
\end{equation*}
\end{enumerate}
Кроме того, полученную Таблицу \ref{tab:bow-bow}  с зависимостью “слово-документ” можно представить в виде произведения матриц “слово-тема” и “тема-документ”. Для этого можно использовать SVD-разложение:

\begin{eqnarray}\label{eq:svd}
{\displaystyle 
{\begin{matrix}

\\&({\textbf {d}}_{j})&&&&&&&\\&\downarrow &&&&&&& \\({\textbf {t}}_{i}^{T})\rightarrow &{
\begin{bmatrix}x_{1,1}&\dots &x_{1,n}\\\\\vdots &\ddots &\vdots \\\\x_{m,1}&\dots &x_{m,n}\\\end{bmatrix}}&=
\\&= {\begin{bmatrix}
{\begin{bmatrix}\,\\\,\\{\textbf {u}}_{1}\\\,\\\,\end{bmatrix}}\dots  
{\begin{bmatrix}\,\\\,\\{\textbf {u}}_{l}\\\,\\\,\end{bmatrix}}
\end{bmatrix}}&\cdot &
   {\begin{bmatrix}\sigma _{1}&\dots &0\\\vdots &\ddots &\vdots \\0&\dots &\sigma _{l}\\\end{bmatrix}}&\cdot &
   {\begin{bmatrix}
   {\begin{bmatrix}&&{\textbf {v}}_{1}&&\end{bmatrix}}\\\vdots \\
   {\begin{bmatrix}&&{\textbf {v}}_{l}&& \end{bmatrix}}
   \end{bmatrix}}
 \end{matrix}}}
\end{eqnarray}

${\textbf {t}}_{i}$  -- слово, ${\textbf {d}}_{i}$ -- документ.\\

Применим такое разложение для текстов из примера и визуализируем (см.~рис.~\ref{fig:svd-example}).
Как можно видеть, получаемые результаты имеют сильную зависимость от корпуса, к которому применяется разложение.

\begin{figure}[h]% p означает, что нужно выделить для рисунка
% отдельную страницу; применяется для больших рисунков
\centering
%Здесь могла быть ваша лягушка.
\includegraphics[width=0.65\textwidth]{img/svd-example.png}
\caption{\label{fig:svd-example}SVD разложение для C1 и С2}
\end{figure}


\subsubsection{Word2vec}

Из-за ряда недостатков, которыми обладали традиционные алгоритмы, исследования в области представлений слов продолжились. В 2013 году Томаш Миколов представил[] подход к кодированию слов, который решал целый ряд проблем, присущих другим моделям, в том числе рост размерности векторного пространства и невозможность сохранять семантическую близость.

Им было представлено 2 подхода, которые он назвал continuous bag-of-words (CBOW) и skip-gram. Они позволяют строить высококачественные распределенные векторные представлений.

Между представленными моделями есть существенное отличие. CBOW вычисляет условную вероятность появления целевого слова исходя из его контекста,  который лежит в окне размера k. В свою очередь Skip-gram предсказывает слова контекста по центральному слову. Предполагается, что контекстные слова расположены симметрично целевым словам на расстоянии, равном размеру окна в обоих направлениях. Схематичное представление данных методов показано на ~рис.~\ref{fig:w2v-models}
\begin{figure}[ht]% p означает, что нужно выделить для рисунка
% отдельную страницу; применяется для больших рисунков
\centering
%Здесь могла быть ваша лягушка.
\includegraphics[width=0.7\textwidth]{img/efficient-models}
\caption{\label{fig:w2v-models}Модели CBOW и Skip-gram (Источник рисунка: Mikolov[])}
\end{figure}

\paragraph{Continuous Bag-of-words}

Рассмотрим упрощенную модель CBOW с контекстом из одного слова более детально (см.~рис.~\ref{fig:w2v-cbow}). Она представляет собой полносвязную нейронную сеть со скрытым слоем. Входной слой, принимающий one-hot вектор, состоит из ${V}$ нейронов. Скрытый слой в свою очередь имеет ${N}$ нейронов. На выходном слое применяется операция Softmax (\ref{gather:softmax}), давая на выходе распределение вероятностей по всем словам в словаре. Слои связаны матрицами весов ${\textbf W} \in \mathcal{R}^{V \times N } $ и ${\textbf W^{'}} \in \mathcal{R}^{H \times V}$ соответственно.

\begin{figure}[ht]% p означает, что нужно выделить для рисунка
% отдельную страницу; применяется для больших рисунков
\centering
%Здесь могла быть ваша лягушка.
\includegraphics[width=0.7\textwidth]{img/CBOW.png}
\caption{\label{fig:w2v-cbow}Модель CBOW (Источник рисунка: Rong~\autocite{Rong2014word2vecPL})}
\end{figure}

Каждое слово из словаря в конечном итоге представляется в виде двух выученных векторов ${\textbf v_c}$ и ${\textbf v_w}$, которые соответствуют контекстному и целевому слову соответственно. Таким образом,  $k$-e слово в словаре будет представлено следующим образом:

\begin{gather}
{\textbf v_c} = {\textbf W_{(k,.)}} \quad и \quad {\textbf v_w} = {\textbf W^{'}_{(.,k)}}
\end{gather}

Тогда для любого слова $w_i$ с контекстным словом $c$ в качестве входных данных получим:
\begin{gather}\label{gather:softmax}
 {p\left({w_i}|{c}\right)} = {\textbf y_i} = \frac{e^{u_i}}{\mathbf{\sum}_{i=1}^{V} e^{u_i}} \quad,\ \ {u_i} = {\mathbf{v_{w_i}^T.v_c}}
\end{gather}


Параметры $ {\mathbf{ \theta}} = {\mathbf{\{v_w, v_c\}}}_{w,c \,\in\, \text{Vocab}}$ обучаются путем оптимизации целевой функции, в качестве которой выступает логарифмическая функция правдоподобия:
\begin{gather}
{\mathbf{ l(\theta)}} = {\mathbf{ \sum_{w \in Vocab} log\left(p\left({w}|{c}\right)\right)}} \\
{ \frac{\partial \mathbf{l(\theta)}}{\partial \mathbf{v_w}}} = { \mathbf{v_c\left(1-p\left({w}Ё{c}\right)\right)}}
\end{gather}

В общем случае модели CBOW (см.~рис.~\ref{fig:cbow-multi}) все One-hot векторы берутся одновременно в качестве входных данных:
\begin{equation}
{\mathbf{h}} = {\mathbf{ W^T(x_1 + x_2 + ... + x_c)}}
\end{equation}

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{img/cbow-multi.pdf}
\caption{\label{fig:cbow-multi}Модель CBOW в общем случае}
\end{figure}

Несмотря на то, что в такой архитектуре не учитывается порядок слов, так как входные векторы суммируются, она сохраняет некоторые семантические характеристики корпусов текста, на которых проводится обучение. В связи с этим схожие по смыслу слова имеют схожие векторные представления. Наиболее популярный пример сохранения семантической близости представлен на ~рис.~\ref{fig:w2v-semantic}.


\begin{figure}[ht]% p означает, что нужно выделить для рисунка
% отдельную страницу; применяется для больших рисунков
\centering
\includegraphics[width=0.7\textwidth]{img/w2v-semantic.png}
\caption{\label{fig:w2v-semantic}Иллюстрация отношений векторного представления слов (Источник рисунка: Mikolov~\autocite{mikolov-etal-2013-linguistic})}
\end{figure}


%
%%
%%
%\newpage
%\subsection{Как вставлять листинги и рисунки}
%
%Для крупных листингов есть два способа. Первый красивый, но в нём не допускается
%кириллица (у вас может встречаться в комментариях и
%печатаемых сообщениях), он представлен на листинге~\ref{list:hwbeauty}.
%\begin{ListingEnv}[H]% буква H означает Here, ставим здесь,
%% элементы, которые нежелательно разрывать обычно не ставят
%% посреди страницы: вместо H используется t (top, сверху страницы),
%% или b (bottom) или p (page, на отдельной странице)
%\begin{lstlisting}
%#include <iostream>
%using namespace std;
%
%int main()
%{
%    cout << "Hello, world" << endl;
%    system("pause");
%    return 0;
%}
%\end{lstlisting}
%%следующую команду для генерации подписи можно опустить,
%% хотя рекомендуется все специальные элементы (таблицы, рисунки,
%% листинги) подписывать. Если подпись пропустить, листинг также не получит
%% номера и на него не сошлёшься в будущем
%\caption{Программа “Hello, world” на \protect\cpp}
%% далее метка для ссылки:
%\label{list:hwbeauty}
%\end{ListingEnv}
%
%Второй не такой красивый, но без ограничений (см.~листинг~\ref{list:hwplain}).
%\begin{ListingEnv}[H]
%\begin{Verb}
%
%#include <iostream>
%using namespace std;
%
%int main()
%{
%    cout << "Привет, мир" << endl;
%}
%\end{Verb}
%\caption{Программа “Hello, world” без подсветки}
%\label{list:hwplain}
%\end{ListingEnv}
%
%Можно использовать первый для вставки небольших фрагментов
%внутри текста, а второй для вставки полного
%кода в приложении, если таковое имеется.
%
%Если нужно вставить совсем короткий пример кода (одна или две строки), то выделение  линейками и нумерация может смотреться чересчур громоздко. В таких случаях можно использовать окружения \texttt{lstlisting} или \texttt{Verb} без \texttt{ListingEnv}. Приведём такой пример с указанием языка программирования, отличного от заданного по умолчанию:
%\begin{lstlisting}[language=Haskell]
%fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
%\end{lstlisting}
%Такое решение~--- со вставкой нумерованных листингов покрупнее
%и вставок без выделения для маленьких фрагментов~--- выбрано,
%например, в книге Эндрю Таненбаума и Тодда Остина по архитектуре
%компьютера~\autocite{TanAus2013} (см.~рис.~\ref{fig:tan-aus}).
%
%Наконец, для оформления идентификаторов внутри строк
%(функция \lstinline{main} и тому подобное) используется
%\texttt{lstinline} или, самое простое, моноширинный текст
%(\texttt{\textbackslash texttt}).
%
%\begin{figure}[p]% p означает, что нужно выделить для рисунка
%% отдельную страницу; применяется для больших рисунков
%\centering
%%Здесь могла быть ваша лягушка.
%\includegraphics[width=\textwidth]{img/tan-aus.png}
%\caption{\label{fig:tan-aus}Пример оформления листингов в~\autocite{TanAus2013}}
%\end{figure}
%
%Использовать внешние файлы (например, рисунки) можно и на \href{http://overleaf.com}{overleaf.com}: ищите кнопочку upload.
%
%\subsection{Как оформить таблицу}
%
%Для таблиц обычно используются окружения table и tabular --- см. таблицу~\ref{tab:widgets}. Внутри окружения tabular используются специальные команды пакета booktabs — они очень красивые; самое главное: использование вертикальных линеек считается моветоном.
%
%\begin{table}
%\centering
%\caption{\label{tab:widgets}Подпись к таблице --- сверху}
%\begin{tabular}{llr}
%\toprule
%\multicolumn{2}{c}{Item} \\
%\cmidrule(r){1-2}
%Животное  & Описание    & Цена (\$) \\
%\midrule
%Gnat      & per gram    & 13.65      \\
%          & each        & 0.01       \\
%Gnu       & stuffed     & 92.50      \\
%Emu       & stuffed     & 33.33      \\
%Armadillo & frozen      & 8.99       \\
%\bottomrule
%\end{tabular}
%\end{table}
%
%\subsection{Как набирать формулы}
%
%\LaTeX{} is great at typesetting mathematics. Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
%$$S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
%      = \frac{1}{n}\sum_{i}^{n} X_i$$
%denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.
%
%\subsection{Как оформлять списки}
%
%Нумерованные списки (окружение enumerate, команды item)…
%
%\begin{enumerate}
%  \item Like this,
%  \item and like this.
%\end{enumerate}
%
%\dots маркированные списки \dots
%
%\begin{itemize}
%  \item Like this,
%  \item and like this.
%\end{itemize}
%
%\dots списки-описания \dots
%
%\begin{description}
%  \item[Word] Definition
%  \item[Concept] Explanation
%  \item[Idea] Text
%\end{description}
%
%\Conc
%
%Помните, что на все пункты списка литературы должны быть ссылки. \LaTeX\ просто не добавит информацию об издании из bib"/файла, если на это издание нет ссылки в тексте. Часто студенты используют в работе  электронные ресурсы: в этом нет ничего зазорного при одном условии: при каждом заимствовании следует ставить соответствующую ссылку. В качестве примера приведём ссылку на сайт нашего института~\autocite{mmcs}.
%
%Для дальнейшего изучения \LaTeX\ рекомендуем книгу Львовского~\autocite{Lvo2003}: она хорошо написана, хотя и несколько устарела.
%Обычно стоит искать подсказки на
%\href{http://tex.stackexchange.com/}{tex.stackexchange.com}, а также
%читать документацию по установленным пакетам с помощью
%команды
%\begin{Verb}
%texdoc имя_пакета
%\end{Verb}
%или на \href{http://ctan.org/}{ctan.org}.

\newpage
% Печать списка литературы (библиографии)
\printbibliography[%{}
    heading=bibintoc%
    %,title=Библиография % если хочется это слово
]
% Файл со списком литературы: biblio.bib
% Подробно по оформлению библиографии:
% см. документацию к пакету biblatex-gost
% http://ctan.mirrorcatalogs.com/macros/latex/exptl/biblatex-contrib/biblatex-gost/doc/biblatex-gost.pdf
% и огромное количество примеров там же:
% http://mirror.macomnet.net/pub/CTAN/macros/latex/contrib/biblatex-contrib/biblatex-gost/doc/biblatex-gost-examples.pdf

\end{document}
