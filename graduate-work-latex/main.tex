% В этом файле следует писать текст работы, разбивая его на
% разделы (section), подразделы (subsection) и, если нужно,
% главы (chapter).

% Предварительно следует указать необходимую информацию
% в файле SETUP.tex

\input{preamble.tex}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\newcommand\simpleparagraph[1]{%
  \stepcounter{paragraph}\paragraph*{\theparagraph\quad{}#1}}
  


\begin{document}
\section*{Перечень условных обозначений}
\label{sec:table}
\addcontentsline{toc}{section}{\nameref{sec:table}}

\textbf{NLP ~--~} Natural Language Processing

\textbf{SVM ~--~} Support Vector Machine

\textbf{POS ~--~} Part Of Speech

\textbf{NER ~--~} Named Entity Recognition

\textbf{SRL ~--~} Semantic Role Labeling

\textbf{OHE ~--~} One-Hot Encoding

\textbf{BOW ~--~} Bag-Of-Words

\textbf{CBOW ~--~} Continuous Bag-Of-Words

\textbf{SVD ~--~} Singular value decomposition

\textbf{CNN ~--~} Convolutional Neural Network


\newpage
\Intro
Принято считать, что история развития обработки естественного языка берёт своё начало в 1950-х годах, когда Алан Тьюринг опубликовал свою работу <<Вычислительные машины и разум>> (англ. Computing Machinery and Intelligence~\cite{turing1950computing}), где был представлен тест Тьюринга.

В настоящий момент в рамках обработки естественного языка (англ. Natural Language Processing, NLP), стоящей на пересечении таких компьютерных наук, как машинное обучение и компьютерная лингвистика, решаются проблемы анализа, понимания и извлечения смысла из естественной человеческой речи.

\textbf{Актуальность темы} дипломной работы обусловлена стремительным ростом рынка онлайн-образования и тем, что изучение иностранных языков является приоритетным направлением для потребителей.
Создатели онлайн-платформ часто называют персонализацию обучения отличительной особенностью их курсов. Однако несмотря на большую гибкость по сравнению с классическим методом изучения иностранного языка, некоторые материалы могут быть устаревшими и неактуальными. Это связано с тем, что процесс формирования упражнений является трудозатратным ввиду задействования человеческого труда и отсутствия программных средств для решения этой задачи.

Рассмотрим основные этапы изучения грамматических явлений при изучении нового языка:

\begin{itemize}
  \item Presentation \\ Демонстрация примеров и структуры грамматической конструкции.
  \item Pattern Recognition\\Самостоятельный поиск грамматических конструкций в исходном тексте.
 \item Controlled Practice\\Построение изучаемых грамматических конструкций в ограниченной форме.
 \item Semi-Controlled Practice\\Персонализация языка с упором на изучаемые грамматические явления.
 \item Free Practice\\Свободное использование языка.
\end{itemize}

\textbf{Цель исследования} - проектирование и реализация системы автоматического формирования заданий видов \emph{Controlled Practice} и \emph{Semi-Controlled Practice}, которая позволит сократить время подготовки методических материалов и дать возможность большей персонализации траектории обучения.

Для достижения результата были поставлены следующие задачи:

\begin{itemize}
  \item изучить методы Data Mining и Machine Learning в области обработки естественного языка; 
  \item изучить методы векторного представления слов;
  \item разработать метод предварительной очистки текста;
  \item изучить модели машинного обучения для извлечения необходимых признаков;
  \item разработать алгоритм поиска грамматических конструкций;
  \item разработать алгоритм формирования заданий на основе найденных конструкций;
  \item реализовать развертывание сервера;
  \item разработать web-платформу для взаимодействия с пользователями;
  \item проанализировать возможности повышения производительности системы;
  \item проанализировать возможности повышения точности обнаружения грамматических конструкций;
  \item реализовать базу данных результатов пользователей для ведения статистики.
\end{itemize}

\textbf{Объектом исследования} дипломной работы является применение компьютерной лингвистики в области онлайн-образования.

\textbf{Предметом исследования} дипломной работы является построение алгоритмов поиска грамматических конструкций и формирования на их основе упражнений по грамматике английского языка с использованием методов компьютерной лингвистики и Machine Learning.

\newpage
% Если typeOfWork в SETUP.tex задан как 2 или 3, то начинать
% надо не с section (раздел), а с главы (chapter)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Теоретические основы задачи обработки естественного языка}
\label{sec:theory}

Задача обработки естественного языка это целый ряд теоретико-мотивированных вычислительных методов, которые позволяют производить анализ человеческого языка. В рамках NLP решается большое количество задач, затрагивающих различные уровни- начиная от определения частей речи, заканчивая созданием диалоговых систем.

Долгое время в задачах NLP применялись модели поверхностного обучения, такие как SVM (англ. Support Vector Machine), обученные на разреженных данных, представленных в пространстве высокой размерности, что не позволяло достичь достаточной точности и требовало значительных вычислительных ресурсов. Однако в последние годы были разработаны новые методы векторного представления слов, что позволило избежать экспоненциального роста размерности. Это помогло сократить время обучения сетей и перейти к методам, основанным на глубоком обучении, которые обеспечивают автоматическое обучение признакам. Это существенно отличает новые архитектуры от тех, что применялись раньше, так как для них больше не требуется ручное конструирование признаков.

Одна из первых архитектур глубокого обучения в области NLP была продемонстрирована Ронаном Коллобертом и Джейсоном Уэстоном \autocite{DBLP:journals/corr/abs-1103-0398}. На тот момент она превосходила большую часть уже существующих моделей в таких задачах, как поиск именованных сущностей и др. С того времени появилось большое количество алгоритмов и моделей, решающих сложные задачи обработки естественного языка.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Стандартные задачи NLP}
\label{subsec:standard}
В области обработки естественного языка выделяют 4 базовые задачи:\emph{ POS tagging (POS), Chunking, Named Entity Recognition (NER), Semantic Role Labeling (SRL)}. Они лежат в основе более высокоуровневых задач, в том числе в задаче автоматизации создания заданий по грамматике. Рассмотрим каждую из них в отдельности.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{POS tagging}
\textbf{POS tagging ~---~} задача маркировки каждого слова в соответствии с принадлежностью его к той или иной части речи: имя существительное, имя прилагательное, глагол и т.д.

POS классификаторы часто базируются на классификаторах, которые обучены на текстовых "окнах". После этого результат передается алгоритму двунаправленного декодирования \autocite{watanabe-sumita-2002-bidirectional}. В качестве признаков может браться контекст слова и некоторые созданные вручную признаки.
Для проверки качества работы модели в задаче POS tagging зачастую применяется размеченный корпус текстов Wall Street Journal (WSJ).
%%%%%%%%%%%%%%%%%%%%
\subsubsection{Chunking}
\textbf{Chunking  ~---~} задача поверхностно-синтаксического анализа. В  ней производится анализ предложения, который сначала индексирует составные части предложений (существительные, глаголы и т.д.), а затем связывает их в единицы более высокого порядка, такие как именные или глагольные группы и т.д.. 

Традиционные алгоритмы поверхностно-синтаксического анализа используют поиск по шаблонам (например, регулярные выражения). Алгоритмы с использованием машинного обучения могут учитывать контекстную информацию, что дает возможность улучшить точность восстановления семантических связей.
%%%%%%%%%%%%%%%%%%%%
\subsubsection{Named Entity Recognition}
\textbf{Named Entity Recognition  ~---~} задача распознавания именованных сущностей. 
Является подзадачей Data Mining, в рамках которой необходимо найти и классифицировать именованные сущности в неструктурированном тексте по заранее заданным категориям, таким как имена людей, названия организаций, количество и т.д.

Распознавание именованных сущностей часто разбивается на две подзадачи: выявление имен и их классификация по типу сущности. Первая фаза рассматривается как проблема сегментации: зачастую именованные сущности рассматриваются как непрерывные промежутки токенов без вложенностей. Для второй фазы требуется определение онтологии, благодаря которой формируются категории вещей.

Существует несколько иерархий именованных сущностей. Так, категории BNN используются в вопросно-ответных системах и включают в себя 29 типов и 64 подтипа. В свою очередь иерархия Sekine является расширенной и состоит из 200 подтипов, а в 2011 Риттер представил иерархию, которая основана на общих типах объектов Freebase. 

Системы NER могут быть основаны как на лингвистических методах грамматики, так и на статистических моделях. Несмотря на то, что системы, созданные вручную, имеют большую точность, они требуют месяцев работы профессиональных лингвистов. Подход с использованием методов машинного обучения сейчас является приоритетным, однако является менее устойчивым и требует большого объема аннотированных данных на этапе обучения моделей. 

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Semantic Role Labeling}
\textbf{Semantic Role Labeling  ~---~} задача маркировки семантических ролей. Представляет собой процесс присваивания меток словам или фразам, которые характеризуют их семантические роли в предложении. Результатом работы является порождение поверхностной интерпретации, основанной на теории семантических ролей.

Процесс часто делят на два этапа: обнаружение семантических аргументов, связанных с предикатом или глаголом, и классификации по их семантическим ролям. 
Процесс часто делят на несколько этапов:  обнаружение и определение значения целевых предикатов, обнаружение и дальнейшую классификацию актантов. 

Для реализации автоматической маркировки используются статистические модели, обученные на размеченных корпусах текстов, содержащих информацию о семантических ролях и предикатах. Примером такого корпуса выступает \emph{PropBank}, который был получен добавлением вручную созданных семантических аннотаций в корпус \emph{Penn Treebank}. Пример такой разметки можно увидеть на Рис.~\ref{fig:redrelex}

\begin{figure}[t]
\centering
\includegraphics[scale=0.5]{img/redrelex.png}
\caption{\label{fig:redrelex}Пример размеченных данных \emph{PropBank}}
\end{figure}

Однако при использовании статистических моделей существует недостаток, который проявляется в зависимости от выбранного аннотированного ресурса. Кроме того, создание семантической разметки является трудозатратной и плохо формализуемой задачей. Из-за этого может проявляться некорректная работа на новых данных. Этот аспект принято называть проблемой доменной специфичности SRL.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Word Embeddings}

Статистические модели стали основным инструментом в задачах NLP, однако в начале они страдали от т.н. проклятия размерности \autocite{bellman1957dynamic}. Это повлекло развитие алгоритмов, которые позволили бы представлять слова в низкоразмерном пространстве.

Стоит сказать, что на данный момент нет общепризнанного перевода термина embedding (от англ. ‘вложение’), поэтому будет использоваться англицизм. 

Embedding представляет собой преобразование некой сущности в числовой вектор. Далее будут рассмотрены основные подходы, применяющиеся для решения данной задачи.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{One-hot encoding}

Одним из первых решений задачи векторного представления слов был так называемый унитарный код. Он представлял собой вектор длины n, которая определяется количеством слов некоторого словаря, содержащий (n-1) нулей и 1 единицу. Индекс значащей единицы соответствовал расположению слова в данном словаре ---см. таблицу~\ref{tab:bow-ohe}.

\begin{table}
\centering
\caption{\label{tab:bow-ohe}One-Hot Encoded векторы}
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
Vocabulary   & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \midrule
bag          & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
words        & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
model        & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
way          & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0  \\
representing & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0  \\
text         & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0  \\
data         & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0  \\
simple       & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0  \\
understand   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0  \\
implement    & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1  \\ \bottomrule
\end{tabular}
\end{table}

Несмотря на то, что такая архитектура позволяет решить проблему кодирования слов, она обладает рядом существенных недостатков:
\begin{itemize}
  \item При добавлении нового слова в середину существующего словаря есть необходимость заново проводить нумерацию его элементов.
  \item Происходит быстрый рост размерности представления текстов. \\ Например для текста из 9 уникальных слов требуется матрица 9х9
 \item Данный метод не предоставляет информации о семантической близости слов. Данный аспект связан с тем, что написание слов не имеет непосредственной связи с объектами, которые они описывают [2.12. Фердинанд де Соссюр (1827–1913). Лингвистический структурализм].
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Bag of Words}
На основе кодирования слов унитарным кодом, рассмотренным в пункте 1.1.1, был предложен более экономичный вариант представления текстов, называемый “мешком слов” (от англ. Bag of words).

Алгоритм построения такого представления содержит следующие основные этапы:
\begin{itemize}
  \item Предварительное создание словаря методом OHE.
  \item Кодирование слов, содержащихся в тексте.
 \item Сложение всех полученных one-hot векторов.
\end{itemize}

На выходе получим числовой вектор, который описывает информацию о количестве различных слов в исходном тексте.
Такая модель не сохраняет структуру входных данных, в связи с чем теряется информация о взаимном расположении слов.
Тем не менее, применяя данный подход можно сравнивать тексты путем сравнения выходных векторов. Примером такой метрики является косинусная мера:

\begin{equation}\label{eq:cos}
{\displaystyle {\text{similarity}}=\cos(\theta )={\mathbf {A} \cdot \mathbf {B}  \over \|\mathbf {A} \|\|\mathbf {B} \|}={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}{\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}},}
\end{equation}

$A_{i}$, $B_{i}$ -- компоненты векторов $\mathbf {A} $ и $ \mathbf {B}$ соответственно.\\

\begin{table}
\centering
\caption{\label{tab:bow-bow}Терм-документная таблица}
\begin{tabular}{@{}lcc@{}}
\toprule
Vocabulary   & \multicolumn{2}{l}{Documents} \\ \midrule
             	    & C1            & C2            \\ \midrule
bag          	    & 1               & 1             \\
words           & 1               & 1             \\
model           & 1               & 1             \\
way              & 1               & 0             \\
representing& 1               & 0             \\
text              & 1               & 0             \\
data             & 1               & 0             \\
simple           & 0               & 1             \\
understand   & 0               & 1             \\
implement    & 0               & 1             \\ \bottomrule
\end{tabular}
\end{table}

Рассмотрим этот процесс имплементации алгоритма.
\begin{enumerate}

  \item Зададим экземпляры текста.\\C1 = \textit{“The Bag of Words model is a way of representing text data.”}\\
	C2 = \textit{“The Bag of Words model is simple to understand and implement.”}
  \item Очистим тексты от пунктуационных символов и ‘стоп-слов’\footnote{the, of, is, a, to, and}, которые не несут смысловой нагрузки. 
   \item Сформируем словарь и закодируем его методом OHE (см. таблицу~\ref{tab:bow-ohe}).

  \item Для каждого предложения сложим One-Hot векторы слов, которые входят в их составы (см. таблицу~\ref{tab:bow-bow}). 
  
  \item Применим косинусную меру \eqref{eq:cos} к полученным векторам, которые количественно описывают исходные тексты.
\begin{equation*}
{\displaystyle {\text{similarity}}=\cos(\theta )= \frac{3} { \sqrt{7} \cdot  \sqrt{6}}= {0.46291}}
\end{equation*}
\end{enumerate}
Кроме того, полученную Таблицу \ref{tab:bow-bow}  с зависимостью “слово-документ” можно представить в виде произведения матриц “слово-тема” и “тема-документ”. Для этого можно использовать SVD-разложение:

\begin{eqnarray}\label{eq:svd}
{\displaystyle 
{\begin{matrix}

\\&({\textbf {d}}_{j})&&&&&&&\\&\downarrow &&&&&&& \\({\textbf {t}}_{i}^{T})\rightarrow &{
\begin{bmatrix}x_{1,1}&\dots &x_{1,n}\\\\\vdots &\ddots &\vdots \\\\x_{m,1}&\dots &x_{m,n}\\\end{bmatrix}}&=
\\&= {\begin{bmatrix}
{\begin{bmatrix}\,\\\,\\{\textbf {u}}_{1}\\\,\\\,\end{bmatrix}}\dots  
{\begin{bmatrix}\,\\\,\\{\textbf {u}}_{l}\\\,\\\,\end{bmatrix}}
\end{bmatrix}}&\cdot &
   {\begin{bmatrix}\sigma _{1}&\dots &0\\\vdots &\ddots &\vdots \\0&\dots &\sigma _{l}\\\end{bmatrix}}&\cdot &
   {\begin{bmatrix}
   {\begin{bmatrix}&&{\textbf {v}}_{1}&&\end{bmatrix}}\\\vdots \\
   {\begin{bmatrix}&&{\textbf {v}}_{l}&& \end{bmatrix}}
   \end{bmatrix}}
 \end{matrix}}}
\end{eqnarray}

${\textbf {t}}_{i}$  -- слово, ${\textbf {d}}_{i}$ -- документ.\\

Применим такое разложение для текстов из примера и визуализируем (см.~рис.~\ref{fig:svd-example}).
Как можно видеть, получаемые результаты имеют сильную зависимость от корпуса, к которому применяется разложение.

%%%%%%%%%%%%%%%%
\begin{figure}[h]
\centering
\includegraphics[width=0.65\textwidth]{img/svd-example.png}
\caption{\label{fig:svd-example}SVD разложение для C1 и С2}
\end{figure}
%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Word2vec}

Из-за ряда недостатков, которыми обладали традиционные алгоритмы, исследования в области представлений слов продолжились. В 2013 году Томаш Миколов представил\autocite{DBLP:journals/corr/abs-1301-3781} подход к кодированию слов, который решал целый ряд проблем, присущих другим моделям, в том числе рост размерности векторного пространства и невозможность сохранять семантическую близость.

Им было представлено 2 подхода, которые он назвал continuous bag-of-words (CBOW) и skip-gram. Они позволяют строить высококачественные распределенные векторные представлений.

Между представленными моделями есть существенное отличие. CBOW вычисляет условную вероятность появления целевого слова исходя из его контекста,  который лежит в окне размера k. В свою очередь Skip-gram предсказывает слова контекста по центральному слову. Предполагается, что контекстные слова расположены симметрично целевым словам на расстоянии, равном размеру окна в обоих направлениях. Схематичное представление данных методов показано на ~рис.~\ref{fig:w2v-models}

%%%%%%%%%%%%%%%%
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{img/efficient-models}
\caption{\label{fig:w2v-models}Модели CBOW и Skip-gram (Источник рисунка: Mikolov \autocite{DBLP:journals/corr/abs-1301-3781})}
\end{figure}
%%%%%%%%%%%%%%%%

\paragraph{Модель CBOW} Рассмотрим упрощенную модель CBOW (см.~рис.~\ref{fig:w2v-cbow}) с контекстом из одного слова более детально. Она представляет собой полносвязную нейронную сеть со скрытым слоем. Входной слой, принимающий one-hot вектор, состоит из ${V}$ нейронов. Скрытый слой в свою очередь имеет ${N}$ нейронов. На выходном слое применяется операция Softmax, давая на выходе распределение вероятностей по всем словам в словаре. Слои связаны матрицами весов ${\textbf W} \in \mathcal{R}^{V \times N } $ и ${\textbf W^{'}} \in \mathcal{R}^{N \times V}$ соответственно.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{img/CBOW.png}
\caption{\label{fig:w2v-cbow}Модель CBOW (Источник рисунка: Rong~\autocite{Rong2014word2vecPL})}
\end{figure}


Каждая строка матрицы ${\textbf W}$ это $N$-мерное векторное представление $\mathbf{v}_w$ связанного слова входного слоя. Формально можно записать строку $i$ матрицы $\textbf{W}$ как $\mathbf{v}_w^T$.
Таким образом, 
\begin{equation}
\mathbf{h} = \mathbf{W}^T \mathbf{x} = \mathbf{W}_{(k, \cdot)}^T := \mathbf{v}_{w_I}^T,
\label{eq:cbow-h}
\end{equation}
где $\mathbf{v}_{w_I}$ - векторное представление входного слова $\mathbf{w_I}$.

Скрытый и выходной слой связаны матрицей $\mathbf{W}' = \{w'_{ij}\}$. Используя данную матрицу весов можно посчитать оценку $u_j$ для каждого слова из словаря
\begin{equation}
u_j = {\mathbf{v}'_{w_j}}^T \mathbf{h},
\label{eq:cbow-uj}
\end{equation}
где  $\mathbf{v}'_{w_j}$ это $j$-я колонка матрицы W'. После этого используется нелинейная функция Softmax(\ref{eq:cbow-yj}) для получения апостериорного распределения слов.
\begin{equation}
p(w_j | w_I) = y_j = \frac{\exp(u_j)}{\sum_{j'=1}^V\exp(u_{j'})},
\label{eq:cbow-yj}
\end{equation}

где $y_j$ является выходом $j$-го нейрона на выходном слое.
Используя формулы (\ref{eq:cbow-h}) и (\ref{eq:cbow-uj}) можно записать (\ref{eq:cbow-yj}) в следующем виде:
\begin{equation}
p(w_j | w_I) = \frac{\exp\left({\mathbf{v}'_{w_j}}^T\mathbf{v}_{w_I}\right)}{\sum_{j'=1}^V\exp\left({\mathbf{v}'_{w_{j'}}}^T\mathbf{v}_{w_I}\right)}
\label{eq:cbow-pwo}
\end{equation}

Учитывая,  что $\mathbf{v}_w$ и $\mathbf{v}'_w$ - представления слова w через строки матрицы $\mathbf{W}$ и колонки матрицы $\mathbf{W'}$ соответственно. Будем называть $\mathbf{v}_w$  «входным вектором», а $\mathbf{v}'_w$ - «выходным вектором» слова w. 

В качестве функции потерь, которую следует минимизировать, используется логарифмическая функция правдоподобия
\begin{equation}
E=-\log p(w_O|w_I)
\label{eq:cbow-loss}
\end{equation}
где $w_I$ - входное слово, а $w_O$- предсказанное.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/cbow-multi.pdf}
\caption{\label{fig:cbow-multi} Модель CBOW в общем случае}
\end{figure}

В общем случае модели CBOW см.~рис.~\ref{fig:cbow-multi}, когда контекст превышает одно слово, при вычислении выходных значений скрытого подсчитывается среднее значение векторов контекста:
\begin{eqnarray}
\mathbf{h} &=& \frac{1}{C}\mathbf{W}^T(\mathbf{x}_1+\mathbf{x}_2+\cdots+\mathbf{x}_C) \\
&=& \frac{1}{C}(\mathbf{v}_{w_1} + \mathbf{v}_{w_2} + \cdots + \mathbf{v}_{w_C})^T
\label{eq:cbow-complex-h}
\end{eqnarray}
где $C$- количество слов в контексте, $w_1, \cdots, w_C$ - контекстные слова, $\mathbf{v}_w$  - входное слово $w$.



\paragraph{Модель Skip-gram}

Рассматриваемая общая модель (см.~рис.~\ref{fig:skip-gram-multi}) является обратной к CBOW. Теперь целевое слово подается на входной слой, а предсказание контекстных слов является результатом работы модели. 

Будем использовать обозначение $\mathbf{v}_{w_I}$ для входного вектора. Через $\mathbf{h}$ обозначим выходные данные скрытого слоя, которые останутся прежними:
\begin{equation}
\mathbf{h} = \mathbf{W}_{(k, \cdot)}^T := \mathbf{v}_{w_I}^T,
\end{equation}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{img/skip-gram-multi.pdf}
\caption{\label{fig:skip-gram-multi} Модель Skip-gram в общем случае}
\end{figure}

Будем использовать обозначение  $\mathbf{v}_{w_I}$ для входного вектора. Через $\mathbf{h}$ обозначим выходные данные скрытого слоя:
\begin{equation}
\mathbf{h} = \mathbf{W}_{(k, \cdot)}^T := \mathbf{v}_{w_I}^T,
\end{equation}

Выходной же слой теперь формирует $C$ полиномиальных распределений вместо одного. Каждый элемент выходного слоя считается с использование матриц скрытого и выходного слоя.

\begin{equation}
p(w_{c,j} = w_{O,c} | w_I) = y_{c,j} = \frac{\exp(u_{c,j})}{\sum_{j'=1}^V\exp(u_{j'})}
\end{equation}
$w_{c,j}$ -- $j$-е слово в $c$-м векторе выходного слоя;\\
$w_{O,c}$  -- действительное $c$-е выходное контекстное слово; \\
$w_I$ -- единственное входное слово;\\
$y_{c,j}$ -- предсказание $j$-го элемента в $c$-м векторе выходного слоя;\\
$u_{c,j}$  -- входные данные $j$-го элемента в $c$-м векторе выходного слоя;\\


Функция потерь также меняется и принимает следующий вид:
\begin{eqnarray}
E &=& -\log p(w_{O,1}, w_{O,2}, \cdots, w_{O,C} | w_I) \\
&=& -\log \prod_{c=1}^C\frac{\exp(u_{c,j_c^*})}{\sum_{j'=1}^V\exp(u_{j'})} \\
&=& -\sum_{c=1}^C u_{j_c^*} + C\cdot\log\sum_{j'=1}^V\exp(u_{j'})
\end{eqnarray}
где  $j_c^*$ -- индекс верного $c$-го выходного слова в словаре.

Таким образом, преимущество использования модели Skip-gram перед CBOW в том, что при том же корпусе текстов получается больше экземпляров обучающей выборки, из-за чего повышается точность предсказаний модели.

Резюмируя, модель word2vec позволяет решить основную проблему векторного представления слов- размерность выходных векторов. Кроме того необходимо отметить главную особенность таких архитектур, которая заключается в том, что несмотря на отсутствие сохранения порядка слов в контексте, так как входные векторы суммируются, они сохраняют некоторые семантические характеристики корпусов текста, на которых проводится обучение. В связи с этим схожие по смыслу слова имеют схожие векторные представления. Наиболее популярный пример сохранения семантической близости представлен на ~рис.~\ref{fig:w2v-semantic}.


\begin{figure}[t]% p означает, что нужно выделить для рисунка
% отдельную страницу; применяется для больших рисунков
\centering
\includegraphics[width=0.7\textwidth]{img/w2v-semantic.png}
\caption{\label{fig:w2v-semantic}Иллюстрация отношений векторного представления слов (Источник рисунка: Mikolov~\autocite{mikolov-etal-2013-linguistic})}
\end{figure}

\subsection{Применение CNN в задачах NLP}
Рассмотренный в предыдущем пункте метод векторного представления слов word2vec и схожие с ним (GloVe, etc.) дали большой толчок в разработке архитектур глубоких нейронных сетей для задач NLP. Данные представления применяются  как первый слой обработки данных в deep learning моделях. 

Одним из методов извлечения признаков из текстовых данных является применение сверточных нейронных сетей (англ. convolutional neural network, CNN).
В задачах NLP наиболее часто встречаются варианты мультизадачных моделей, в которых результаты работы низкоуровневых слоев служат входными данными (признаками) для более высокоуровневых. Так, например, корректные POS тэги слов помогают улучшить показатель точности на задачах построения дерева зависимости. 

Пример такой архитектуры был представлен авторами Коллобертом и Уэстоном \autocite{DBLP:journals/corr/abs-1103-0398}. В своей работе они использовать мультизадачную нейронную сеть, которая решает задачи определения частей речи, восстановления семантических ролей, поиск именованных сущностей и др. 

\begin{figure}[t]

\centering
\includegraphics[width=0.45\textwidth]{img/collobertCNN.pdf}
\caption{\label{fig:collobertCNN}Пример CNN для задачи классификации на уровне слов (Источник рисунка: Collobert~\autocite{DBLP:journals/corr/abs-1103-0398})}
\end{figure}

Как видно из Рис.~\ref{fig:collobertCNN}, в этой архитектуре была использована таблица поиска (англ. lookup table) для векторного представления слов, благодаря которой последовательность слов $\{s_1, s_2, ... s_n\}$ была преобразована в некоторую последовательность векторов $\{ {{w}_{s_1}}, {{w}_{s_2}}, ... {{w}_{s_n}} \}$

Сверточные нейронные сети способны извлекать информацию из n-грамм входного предложения, что дает возможность создания информативного семантического представления для последующих задач. Рассмотрим несколько архитектур и применений CNN для обработки естественного языка.

\subsubsection{Предсказание на уровне предложений}
\begin{figure}[t]
	\includegraphics[scale=0.45]{img/CNN}
	\centering
	\caption{Модель CNN}\label{fig:CNN}
\end{figure}
Положим ${\mathbf w_{i}} \in \mathcal{R}^d$ - векторное представление $i$-го слова входного предложения, где $d$ - размерность векторов вложений. Тогда предложение, состоящее из n слов, может быть представлено матрицей вложений слов ${\mathbf W} \in \mathcal{R}^{n \times d}$. Пример такого представления можно увидеть на ~Рис.~\ref{fig:CNN}.

Обозначим ${\mathbf w_{i:i+j}} $ как конкатенацию векторов
 вложений слов ${\mathbf w_{i}}, {\mathbf w_{i+1}}, ... {\mathbf w_{i+j}}$. Начальная свертка применяется к этому входному слою и представляет собой фильтр ${\mathbf k} \in \mathcal{R}^{h \times d}$, который применяется к h словам, создавая новые признаки. Например, признак $c_i$ получается путем применения окна свертки к словам ${\mathbf w_{i:i+h-1}} $
\begin{equation}
c_i = f({\mathbf w_{i:i+h-1}}.{\mathbf k}^T + b )
\end{equation} 
 $b \in \mathcal{R}$ - смещение, $f$ - нелинейная функция активации (например, сигмоида). 
 
 Фильтр $k$ применяется ко всем возможным окнам текста и формирует карту признаков
\begin{equation}
c = [c_1, c_2, ... , c_{n-h+1}]
\end{equation} 

Стоит отметить, что представление текстовых данных отличается от представления изображений, к которым изначально применялись CNN. В отличие от них, текстовые данные представлены в двух, а не трех измерениях: ширине и количестве каналов. Ширину формирует количество слов в последовательности, а количество каналов - длина векторного представления слов. Учитывая то, что embedding слов сохраняет смысл только полностью, в задачах NLP применяются одномерные свертки (1D convolution), двигаясь только по оси, которая отвечает за количество слов.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{img/maxpool}
\caption{\label{fig:maxpool}Операция \emph{max-pooling}}
\end{figure}

В CNN применяется множество фильтров различной ширины, что позволяет извлекать специфичную информацию из различных n-грамм. Чаще всего после сверточного слоя идет слой с операцией \emph{max-pooling}, которую обозначим $\hat{c} = max\{c\}$ (см.~рис.~\ref{fig:maxpool}). Эта операция выбирает максимальное значение из некоторого окна, проходясь по карте признаков. У применения такой операции есть несколько предпосылок.
Первая  -  \emph{max-pooling} обеспечивает контроль размерности выходных данных, что необходимо для задач классификации. Вторая причина заключается в том, что операция  \emph{max-pooling} позволяет более высокоуровневым слоям работать с информацией, которая захватывает больший участок данных, при этом такая операция передает наиболее значимые данные, полученные на всем предложении. 

Комбинации слоев свертки и \emph{max-pooling} выступают основными составляющими сверточной нейронной сети. Их последовательное применение совместно с большим количеством ядер свертки помогают улучшить качество анализа предложения и создавать абстрактные представления, содержащие семантическую информацию. При этом операции свертки на более глубоких слоях захватывают большую часть предложения. Такие операции продолжаются до тех пор, пока не захватят все предложение и не создадут обобщение его характеристик.

\subsubsection{Предсказание на уровне слов}
\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{img/network-window}
\caption{\label{fig:network-window}Модель Window Approach}
\end{figure}
Рассмотренная архитектура позволяет делать предсказания на уровне предложений, однако такой подход не решает ряд задач, в том числе описанных в пункте \ref{subsec:standard}. Для них необходимо производить предсказания на уровне слов. Адаптация CNN под такие задачи заключается в применении т.н. "оконного подхода" (англ. Window Approach).  Он основывается на гипотезе, что классификация слова зависит в первую очередь от слов, которые его окружают.

Таким образом, к каждому слову применяется окно фиксированного размера $2n + 1$, где целевое слово располагается в центре. Выделенный фрагмент теста рассматривается как под-предложение. Далее к нему применяется многослойная нейронная сеть и производится предсказание некоторых признаков целевого слова. Общая архитектура такой сети представлена на Рис.~\ref{fig:network-window}. Следует отметить, что она рассматривает слова как последовательности, что особенно актуально для таких задач, как SRL. 


%
%%
%%
%\newpage
%\subsection{Как вставлять листинги и рисунки}
%
%Для крупных листингов есть два способа. Первый красивый, но в нём не допускается
%кириллица (у вас может встречаться в комментариях и
%печатаемых сообщениях), он представлен на листинге~\ref{list:hwbeauty}.
%\begin{ListingEnv}[H]% буква H означает Here, ставим здесь,
%% элементы, которые нежелательно разрывать обычно не ставят
%% посреди страницы: вместо H используется t (top, сверху страницы),
%% или b (bottom) или p (page, на отдельной странице)
%\begin{lstlisting}
%#include <iostream>
%using namespace std;
%
%int main()
%{
%    cout << "Hello, world" << endl;
%    system("pause");
%    return 0;
%}
%\end{lstlisting}
%%следующую команду для генерации подписи можно опустить,
%% хотя рекомендуется все специальные элементы (таблицы, рисунки,
%% листинги) подписывать. Если подпись пропустить, листинг также не получит
%% номера и на него не сошлёшься в будущем
%\caption{Программа “Hello, world” на \protect\cpp}
%% далее метка для ссылки:
%\label{list:hwbeauty}
%\end{ListingEnv}
%
%Второй не такой красивый, но без ограничений (см.~листинг~\ref{list:hwplain}).
%\begin{ListingEnv}[H]
%\begin{Verb}
%
%#include <iostream>
%using namespace std;
%
%int main()
%{
%    cout << "Привет, мир" << endl;
%}
%\end{Verb}
%\caption{Программа “Hello, world” без подсветки}
%\label{list:hwplain}
%\end{ListingEnv}
%
%Можно использовать первый для вставки небольших фрагментов
%внутри текста, а второй для вставки полного
%кода в приложении, если таковое имеется.
%
%Если нужно вставить совсем короткий пример кода (одна или две строки), то выделение  линейками и нумерация может смотреться чересчур громоздко. В таких случаях можно использовать окружения \texttt{lstlisting} или \texttt{Verb} без \texttt{ListingEnv}. Приведём такой пример с указанием языка программирования, отличного от заданного по умолчанию:
%\begin{lstlisting}[language=Haskell]
%fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
%\end{lstlisting}
%Такое решение~--- со вставкой нумерованных листингов покрупнее
%и вставок без выделения для маленьких фрагментов~--- выбрано,
%например, в книге Эндрю Таненбаума и Тодда Остина по архитектуре
%компьютера~\autocite{TanAus2013} (см.~рис.~\ref{fig:tan-aus}).
%
%Наконец, для оформления идентификаторов внутри строк
%(функция \lstinline{main} и тому подобное) используется
%\texttt{lstinline} или, самое простое, моноширинный текст
%(\texttt{\textbackslash texttt}).
%
%\begin{figure}[p]% p означает, что нужно выделить для рисунка
%% отдельную страницу; применяется для больших рисунков
%\centering
%%Здесь могла быть ваша лягушка.
%\includegraphics[width=\textwidth]{img/tan-aus.png}
%\caption{\label{fig:tan-aus}Пример оформления листингов в~\autocite{TanAus2013}}
%\end{figure}
%
%Использовать внешние файлы (например, рисунки) можно и на \href{http://overleaf.com}{overleaf.com}: ищите кнопочку upload.
%
%\subsection{Как оформить таблицу}
%
%Для таблиц обычно используются окружения table и tabular --- см. таблицу~\ref{tab:widgets}. Внутри окружения tabular используются специальные команды пакета booktabs — они очень красивые; самое главное: использование вертикальных линеек считается моветоном.
%
%\begin{table}
%\centering
%\caption{\label{tab:widgets}Подпись к таблице --- сверху}
%\begin{tabular}{llr}
%\toprule
%\multicolumn{2}{c}{Item} \\
%\cmidrule(r){1-2}
%Животное  & Описание    & Цена (\$) \\
%\midrule
%Gnat      & per gram    & 13.65      \\
%          & each        & 0.01       \\
%Gnu       & stuffed     & 92.50      \\
%Emu       & stuffed     & 33.33      \\
%Armadillo & frozen      & 8.99       \\
%\bottomrule
%\end{tabular}
%\end{table}
%
%\subsection{Как набирать формулы}
%
%\LaTeX{} is great at typesetting mathematics. Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
%$$S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
%      = \frac{1}{n}\sum_{i}^{n} X_i$$
%denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.
%
%\subsection{Как оформлять списки}
%
%Нумерованные списки (окружение enumerate, команды item)…
%
%\begin{enumerate}
%  \item Like this,
%  \item and like this.
%\end{enumerate}
%
%\dots маркированные списки \dots
%
%\begin{itemize}
%  \item Like this,
%  \item and like this.
%\end{itemize}
%
%\dots списки-описания \dots
%
%\begin{description}
%  \item[Word] Definition
%  \item[Concept] Explanation
%  \item[Idea] Text
%\end{description}
%
%\Conc
%
%Помните, что на все пункты списка литературы должны быть ссылки. \LaTeX\ просто не добавит информацию об издании из bib"/файла, если на это издание нет ссылки в тексте. Часто студенты используют в работе  электронные ресурсы: в этом нет ничего зазорного при одном условии: при каждом заимствовании следует ставить соответствующую ссылку. В качестве примера приведём ссылку на сайт нашего института~\autocite{mmcs}.
%
%Для дальнейшего изучения \LaTeX\ рекомендуем книгу Львовского~\autocite{Lvo2003}: она хорошо написана, хотя и несколько устарела.
%Обычно стоит искать подсказки на
%\href{http://tex.stackexchange.com/}{tex.stackexchange.com}, а также
%читать документацию по установленным пакетам с помощью
%команды
%\begin{Verb}
%texdoc имя_пакета
%\end{Verb}
%или на \href{http://ctan.org/}{ctan.org}.

% Печать списка литературы (библиографии)
\newpage

\printbibliography[%{}
    heading=bibintoc%
    %,title=Библиография % если хочется это слово
]
% Файл со списком литературы: biblio.bib
% Подробно по оформлению библиографии:
% см. документацию к пакету biblatex-gost
% http://ctan.mirrorcatalogs.com/macros/latex/exptl/biblatex-contrib/biblatex-gost/doc/biblatex-gost.pdf
% и огромное количество примеров там же:
% http://mirror.macomnet.net/pub/CTAN/macros/latex/contrib/biblatex-contrib/biblatex-gost/doc/biblatex-gost-examples.pdf

\end{document}
